{"cells":[{"cell_type":"markdown","source":["![stream](https://image.slidesharecdn.com/hl7v2messagingconformancejan2011-110303095217-phpapp02/95/hl7-v2-messaging-conformance-jan-2011-15-728.jpg?cb=1299146151)\n\n#### Author: Vedant Jain\n\n##### Medical Coding\nA coder’s job is to code to the highest level of specificity. This means abstracting the most information out of the medical reports from the provider and taking accurate notes. It also means knowing the medical terminology for both procedures and diagnoses. Coding to a general level, or undercoding (which we’ll discuss in a moment) can lead to a rejected or denied claim.\n\n##### LOINC – Logical Observation Identifiers Names and Codes\nLogical Observation Identifiers Names and Codes (LOINC) was created in 1994 by the Regenstrief Institute as a free, universal standard for laboratory and clinical observations, and to enable exchange of health information across different systems. Where ICD records diagnoses and CPT services, LOINC is a code system used to identify test observations. LOINC codes are often more specific than CPT, and one CPT code can have multiple LOINC codes associated with it.\n\nCurrently, more than 26,000 people in 157 countries are using LOINC, and it has been recognized as the preferred standard for coding testing and observations in HL7.\n\n\n###### With the following solution built on top of Spark, a healthcare provider benefitted in the following ways:\n* ######Timely and accurate billing process resulting in 1M +  additional annual revenue from documented secondary diagnoses and care \n* ######Reduced costs with Improved surgery suite/staff utilization\n* ######Average of 50,000 documents passed through annotators per day versus 5,000 historically\n* ######Free-text search use cases completing in milliseconds versus 30+ minutes previously\n* ######NLP, Categorization & Text Search for HL7/EMR Data"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/vedant-demo/healthcare/hl7"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["path = \"/mnt/vedant-demo/healthcare/hl7/\"\nhl7RDD = sc.wholeTextFiles(path)\nhl7DF = hl7RDD.toDF()\ndisplay(hl7DF)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["###Step 1: Parse every HL7 message into JSON\n###### Convert the HL7 payload into JSON. The hidden cell below contains a python function that does the conversion"],"metadata":{}},{"cell_type":"markdown","source":["Hidden Cell Below"],"metadata":{}},{"cell_type":"code","source":["import sys, hl7, json\n\nimport sys, hl7, json\nfrom datetime import datetime\nimport pandas as pd\nimport time\nimport re\n\n\n\ndef to_date(dt):\n    try:\n        date = datetime.strptime(dt , '%Y%m%d')\n        return date\n    except:\n        return None\n\ndef get_pname(seg):\n    try:    \n        name = str(seg[5])\n        name = name.replace(\"^\", \" \")\n    \n    except:\n        name = None\n    \n    return name\n\ndef get_paddress(seg):\n    try:\n        address = str(seg[11]).replace(\"^\", \",\")\n        address = address.split(',')\n        address = str(address[:4]).replace(\"[\", \"\").replace(\"]\", \"\")\n            \n    except:\n        address  = None\n    \n    return address\n    \ndef get_segments(message):\n  num_ids = len(message)\n  seg = []\n  for i in xrange(1, num_ids+1):    \n    seg.append(message['segments'][i]['_id'])\n  return seg\n\ndef cleanup_json(thisjson):\n    clean_json = re.sub(r':\\s*{', ': {', thisjson)\n    clean_json = re.sub(r'}\\n,', '},', clean_json)\n    clean_json = re.sub(r',(\\s*)}', r'\\1}', clean_json)\n    clean_json = thisjson.replace(\"'\", \"\\\\\").replace(\"\\\\\",\"\")\n    return clean_json\n\n  \ndef readHL7(infile):\n    firstLine = True\n    Messages = []\n    strMessage = ''\n    for line in infile:\n#         line = line.strip()\n        if(len(line) > 0):\n             if(firstLine == False and line.startswith(u'MSH|^~\\&')):\n                 Messages.append(strMessage)\n                 strMessage = line\n             else:\n                 strMessage += line\n             firstLine = False\n    if(len(strMessage) >= 4):\n         Messages.append(strMessage)\n    Messages = str(Messages)\n    for message in infile:\n        h = hl7.parse(infile)\n    try:\n      msh_10 = h.segment('MSH')[10];\n      #event = h.segment('MSH')[9];\n      msgdt = str(h.segment('MSH')[7]);  \n      msgdt = to_date(msgdt[:8])\n      _segments = [];\n      segIndex = 1;\n      segDic = {};\n      segARR = []\n    except:\n      msh_10 = None;\n      #event = h.segment('MSH')[9];\n      msgdt = None \n      _segments = [];\n      segIndex = 1;\n      segDic = {};\n      segARR = []\n    try:\n        pname = get_pname(h.segment('PID'))\n    except:\n        pname = None\n    \n    try:\n        paddress = get_paddress(h.segment('PID'))\n    except:\n        paddress = None\n    \n    for seg in h:\n        segName = unicode(seg[0])\n        segVal = unicode(seg)\n        fieldIndex = 1\n        fieldCount = 1\n        _fields = []\n        seg.pop(0)\n        if(segName == 'MSH'):\n            fieldDoc = {'_id':'MSH_1','Val': seg.separator}\n            _fields.append(fieldDoc)\n            fieldCount += 1\n            fieldIndex += 1\n\n        for field in seg:\n            fieldName = segName+'_'+unicode(fieldIndex)\n            fieldVal = unicode(field)\n            hasRepetitions = False;\n            if fieldVal:\n                fieldDoc = {'_id': fieldName,'Val': fieldVal}\n                \n                if ('~' in fieldVal and fieldName != 'MSH_2'):\n                    hasRepetitions = True;\n                    _repfields = []\n                    repFields = fieldVal.split('~');\n                    repIndex = 1;\n                    for repField in repFields:\n                        if repField:\n                            repFieldVal = unicode(repField);\n                            fieldName = segName+'_'+unicode(fieldIndex)\n                            fieldDoc = {'_id': fieldName,'Val': repFieldVal, 'Rep': repIndex}\n                            _repfields.append(fieldDoc)\n                        \n                            if('^' in repFieldVal):\n                                repFieldComps = repFieldVal.split('^');\n                                comIndex = 1;\n                                for repFieldComp in repFieldComps:\n                                    repFieldCompVal = unicode(repFieldComp);\n                                    comName = segName+'_'+unicode(fieldIndex)+'_'+unicode(comIndex)\n                                    if repFieldCompVal:\n                                        fieldDoc = {'_id': comName,'Val': repFieldCompVal, 'Rep': repIndex}\n                                        _repfields.append(fieldDoc)\n                                    comIndex += 1\n                        repIndex += 1;\t\n\t\t\t\t\t\t\t\n                    fieldDoc = {'_id': fieldName,'Val': fieldVal, 'Repetitions': _repfields}\n\t\t\t\t\t\n                _fields.append(fieldDoc)\n                fieldCount += 1\n\t\t\t\t\n                if (hasRepetitions == False and len(field) > 1 and fieldName != 'MSH_2'):\n                    comIndex = 1\n                    for component in field:\n                        comName = segName+'_'+unicode(fieldIndex)+'_'+unicode(comIndex)\n                        comVal = unicode(component)\n                        if comVal:\n                            fieldDoc = {'_id': comName,'Val': comVal}\n                            _fields.append(fieldDoc)\n                        comIndex += 1\n            fieldIndex += 1\n\n        if segName in segDic:\n            segDic[segName] = segDic[segName] + 1;\n        else:\n            segDic[segName] = 1;\n\t\t\n        segDoc ={'_id': segName, 'Rep': segDic[segName], 'Seq': segIndex, 'Val': segVal, 'FC': fieldIndex-1, 'VF': fieldCount-1, 'Fields': _fields}\n        _segments.append(segDoc)\n        segIndex += 1\n        #segARR.append(segName)\n        #segJ = [json.dumps(x) for x in segARR]\n        \n    json_segments = [json.dumps(x) for x in _segments]\n    \n    \n    #json_segments = cleanup_json(json_segments)\n    #ts = time.time()\n    hl7doc = ('{ \"id\": \"%s\", \"date\": \"%s\", \"name\": \"%s\", \"address\": \"%s\", \"segments\": %s }') % (msh_10, msgdt, pname, paddress, json_segments)\n    hl7doc = cleanup_json(hl7doc)\n    hl7js = json.dumps(hl7doc)\n    newjson = json.loads(hl7js)\n    return newjson\n#         doc = hl72JSON(h)     \n#         return h\n#         newjson = json.loads(doc)\n#         return newjson\n#         return message\n\n# hl7RDD.collect()\n# display(hl7DF)\n# hl7RDD.map(lambda x: x[1].split(\",\"))\n# hl7DF = hl7RDD.toDF()\n# display(hl7DF)\n\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["path = \"/mnt/vedant-demo/healthcare/hl7/\"\nhl7RDD = sc.wholeTextFiles(path)\nhl7RDD = hl7RDD.map(lambda x: readHL7(x[1]))\n# hl7RDD.collect()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Step 2: Create a JSON Dataframe"],"metadata":{}},{"cell_type":"code","source":["hl7DF = spark.read.json(hl7RDD)\ndisplay(hl7DF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["patientDF  = hl7DF.selectExpr(\"id\",\"name\",\"address\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(patientDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["###Step 3: Get the data from individual segments and apply structure"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\ndfsSegments = hl7DF.select(\"address\", \"date\", \"id\", \"name\", explode(\"segments\").alias(\"segmentsData\"))\n# display(dfsSegments)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["##### Chart Patient Treatment Timeline\n\nBy creating a structured dataset keyed on patient information and organized by timestamp we can create a view of the patient's treatment timeline. Similarly, by keying on the condition across all patients, we can create a view of outcomes of treatments."],"metadata":{}},{"cell_type":"code","source":["dfsSegmentsData = dfsSegments.select(dfsSegments[\"address\"], dfsSegments[\"date\"], dfsSegments[\"id\"], dfsSegments[\"name\"], dfsSegments[\"segmentsData\"].getField(\"FC\"), dfsSegments[\"segmentsData\"].getField(\"Rep\"), dfsSegments[\"segmentsData\"].getField(\"Seq\"), dfsSegments[\"segmentsData\"].getField(\"VF\"), dfsSegments[\"segmentsData\"].getField(\"Val\").alias(\"values\"), dfsSegments[\"segmentsData\"].getField(\"_id\"))\n\ndisplay(dfsSegmentsData.filter(dfsSegmentsData.date != 'None'))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# dfsSegmentsData.write.mode(\"overwrite\").parquet(\"/mnt/vedant-demo/healthcare/hl7-parsed\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Step 4: Extract Lab Results"],"metadata":{}},{"cell_type":"markdown","source":["The Observation/Result segment is used to transmit the observations of the LAB. OBX segments have great flexibility to report information. When properly coded, OBX segments report a large amount of information in a small amount of space. OBX segments within the ORU message are widely used to report laboratory and other clinical information.\n\nThere can be many OBX segments identified like OBX|1|, OBX|2|, OBX|3|, OBX|4|, OBX|5|, and OBX|6|, etc.\n\nAn Example of information contained in the OBX segment"],"metadata":{}},{"cell_type":"markdown","source":["![stream](http://lh6.ggpht.com/-fWCUtlqIvBg/U_gzX7cyonI/AAAAAAAAFH8/rEgY-kghFzU/image_thumb%25255B1%25255D.png?imgmax=800)\n\nFor details on the OBX Segment and other HL7 segments, please refer to the following guide: https://www.hcup-us.ahrq.gov/datainnovations/clinicalcontentenhancementtoolkit/hi6.jsp"],"metadata":{}},{"cell_type":"markdown","source":["####Filter the dataframe to retrieve only the OBX data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndf = spark.read.parquet(\"/mnt/vedant-demo/healthcare/hl7-parsed/\")\ndf = df.select(\"*\").filter(df[\"`segmentsData._id`\"] == \"OBX\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.sql.functions import split\nsplit_col = split(df['values'], '\\\\|')\ndf.withColumn(\"values\", split(\"values\", \"\\\\|\"))\ndf = df.withColumn(\"reason_for_visit\", split_col.getItem(5))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Step 5: Perform data cleansing - remove stop words, special characters etc.."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n\ndef cleaning(value):\n  value = value.replace(\"_\", \" \")\n  return value\n\nclean_udf = udf(cleaning)\n\nnewdf = df.withColumn(\"reason_for_visit\", clean_udf(df.reason_for_visit))\n\n# display(newdf)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql.types import ArrayType\nimport struct\nimport re\n\ndef cleanup_text(record):\n    words = record.split()\n    \n    # Default list of Stopwords\n    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n    u'can', 'cant', 'come', u'could', 'couldnt', \n    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n    u'each', \n    u'few', 'finally', u'for', u'from', u'further', \n    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n    u'just', \n    u'll', \n    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n    u'no', u'nor', u'not', u'now', \n    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n    u'r', u're', \n    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n    u'under', u'until', u'up', \n    u'very', \n    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n    \n    # Custom List of Stopwords - Add your own here\n    stopwords_custom = ['patient', 'complained', 'reason', 'visit']\n    stopwords = stopwords_core + stopwords_custom \n    words = [re.sub('[^a-zA-Z0-9]','',word) for word in words]                                       # Remove special characters\n    final = [word.lower() for word in words if word.lower() not in stopwords]     # Remove stopwords \n    final = str(final)\n    final = final.replace(\"[\", \"\").replace(\"]\", \"\")\n    return final\n \nudf_cleantext = udf(cleanup_text , StringType())\nfinaldf = newdf.withColumn(\"items\", udf_cleantext(newdf.reason_for_visit))\ndisplay(finaldf)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["###### Now we have our test data properly cleaned and ready for machine learning."],"metadata":{}},{"cell_type":"markdown","source":["### Step 6: Prepare the training dataset. We will use the LOINC data to train our LDA Model."],"metadata":{}},{"cell_type":"code","source":["import re\ndef cleaned_names(record):\n  items = []\n#   record = [x for x in record]\n  if record is not None:\n    record = record.encode('utf-8')\n    record = str(record)\n    names = record.split(\";\")\n    for x in names:\n      items.append(x)\n#     names = [x.split(\";\") for x in record]\n#     names = record[0]\n  else: \n    names = None\n  items = [re.sub('[^a-zA-Z0-9]','',word) for word in items]                                       # Remove special characters\n  items = str(items)\n  items = items.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\")\n  return items\ncleaned_names_udf = udf(cleaned_names)\ndf = spark.read.option(\"header\", \"true\").csv(\"/mnt/vedant-demo/healthcare/loinc-csv/\")\ndf = df.withColumn(\"items\", cleaned_names_udf(df.RELATEDNAMES2))\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["#### Let's lookup all the terms that are related to 'Alcohol' and their status"],"metadata":{}},{"cell_type":"code","source":["display(df.where(df.RELATEDNAMES2.like(\"%alcohol%\")).filter(df.STATUS != \"which are found in whole blood\"))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["##### Similarily, for hygiene:"],"metadata":{}},{"cell_type":"code","source":["display(df.where(df.RELATEDNAMES2.like(\"%hygiene%\")).filter(df.STATUS != \"which are found in whole blood\"))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### Step 7: Create a ML pipeline with Tokenizer, CountVectorizer and LDA\n###### In this step, we will train algorithm to automatically generate/recommend a list of LOINC codes based on the conditions described in the observational results"],"metadata":{}},{"cell_type":"markdown","source":["#### Topic Modeling Latent Dirichlet Allocation"],"metadata":{}},{"cell_type":"markdown","source":["![stream](https://littleml.files.wordpress.com/2016/11/topic_graph.png?w=497&h=185)\n\nIn LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]\n\nFor example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as \"CAT_related\". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.\n\nEach document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.\n[SOURCE: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation]"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.ml.classification import NaiveBayes\n\n\ntokenizer = Tokenizer(inputCol=\"items\", outputCol=\"words\")\nloincData = tokenizer.transform(df)\n# display(loincData)\n# # Option 1 - HashingTF\n# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n# featurizedData = hashingTF.transform(loincData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n# Option 2 (CountVectorizer) - Term Frequency Vectorization    : \ncv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 100000)\ncvmodel = cv.fit(loincData)\nfeaturizedData = cvmodel.transform(loincData)\nvocab = cvmodel.vocabulary\nvocab_broadcast = sc.broadcast(vocab)\n\n# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n# idf = IDF(inputCol=\"ngramFeatures\", outputCol=\"features\") # for n-gram features\n# idfModel = idf.fit(featurizedData)\n\nlda = LDA(k = 4, featuresCol=\"rawFeatures\")\nldamodel = lda.fit(featurizedData)\npipeline = Pipeline(stages=[tokenizer, cv, lda])\nmodel = pipeline.fit(df)\n\nrescaledData = ldamodel.transform(featurizedData)\n\ndisplay(rescaledData)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### Step 8: Use pipeline to transform the test and the training dataset"],"metadata":{}},{"cell_type":"code","source":["ldaResults = model.transform(finaldf)\n# display(ldaResults)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["from pyspark.ml.clustering import LDA\nldatopics = ldamodel.describeTopics()\n\ndef map_termID_to_Word(termIndices):\n    words = []\n    for termID in termIndices:\n        words.append(vocab_broadcast.value[termID])\n    \n    return words\n\nudf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\nldatopics_mapped = ldatopics.withColumn(\"description\", udf_map_termID_to_Word(ldatopics.termIndices))\n\ndisplay(ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.description))\n"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["### Step 9: Evaluate the Model"],"metadata":{}},{"cell_type":"code","source":["# Lower the perplexity and higher the log likelihood, better the model\nll = ldamodel.logLikelihood(rescaledData)\nlp = ldamodel.logPerplexity(rescaledData)\nprint(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\nprint(\"The upper bound on perplexity: \" + str(lp))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# Lower the perplexity and higher the log likelihood, better the model\nll = ldamodel.logLikelihood(ldaResults)\nlp = ldamodel.logPerplexity(ldaResults)\nprint(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\nprint(\"The upper bound on perplexity: \" + str(lp))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["from pyspark.sql.types import FloatType\nfrom pyspark.sql.functions import lit\ndef breakout_array(index_number, record):\n    vectorlist = record.tolist()\n    return vectorlist[index_number]\n\nudf_breakout_array = udf(breakout_array, FloatType())\nenrichedData = ldaResults                                                                   \\\n        .withColumn(\"topic_1\", udf_breakout_array(lit(0), ldaResults.topicDistribution))  \\\n        .withColumn(\"topic_2\", udf_breakout_array(lit(1), ldaResults.topicDistribution))  \\\n        .withColumn(\"topic_3\", udf_breakout_array(lit(2), ldaResults.topicDistribution))  \\\n        .withColumn(\"topic_4\", udf_breakout_array(lit(3), ldaResults.topicDistribution)) \nenrichedData.createOrReplaceTempView(\"ldaResultsFromVisits\")"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from pyspark.sql.types import FloatType\nfrom pyspark.sql.functions import lit\ndef breakout_array(index_number, record):\n    vectorlist = record.tolist()\n    return vectorlist[index_number]\n\nudf_breakout_array = udf(breakout_array, FloatType())\nenrichedData = rescaledData                                                                   \\\n        .withColumn(\"topic_1\", udf_breakout_array(lit(0), rescaledData.topicDistribution))  \\\n        .withColumn(\"topic_2\", udf_breakout_array(lit(1), rescaledData.topicDistribution))  \\\n        .withColumn(\"topic_3\", udf_breakout_array(lit(2), rescaledData.topicDistribution))  \\\n        .withColumn(\"topic_4\", udf_breakout_array(lit(3), rescaledData.topicDistribution)) \nenrichedData.createOrReplaceTempView(\"ldaResults\")"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%sql select * from ldaresults"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["%sql \n\nSELECT LOINC_NUM, RELATEDNAMES2, COMPONENT,\n    CASE\n        WHEN Topic_1 >= Topic_2 AND Topic_1 >= Topic_3 AND Topic_1 >= Topic_4 THEN \"Topic_1\" \n        WHEN Topic_2 >= Topic_1 AND Topic_2 >= Topic_3 AND Topic_2 >= Topic_4 THEN \"Topic_2\"\n        WHEN Topic_3 >= Topic_1 AND Topic_3 >= Topic_2 AND Topic_3 >= Topic_4 THEN \"Topic_3\"\n        WHEN Topic_4 >= Topic_1 AND Topic_4 >= Topic_2 AND Topic_4 >= Topic_3 THEN \"Topic_4\"\n        ELSE NULL\n    END AS ISSUE\n    FROM LDARESULTS"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["%sql SELECT A.ID, A.NAME, A.DATE, A.REASON_FOR_VISIT, B.LOINC_NUM FROM VISITS A JOIN LOINC B ON A.ISSUE == B.ISSUE"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["### Step 10: Get recommendations"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nvisits_lda = spark.sql('SELECT ID, NAME, DATE, REASON_FOR_VISIT, CASE \\\n        WHEN Topic_1 >= Topic_2 AND Topic_1 >= Topic_3 AND Topic_1 >= Topic_4 THEN \"Topic_1\"  \\\n        WHEN Topic_2 >= Topic_1 AND Topic_2 >= Topic_3 AND Topic_2 >= Topic_4 THEN \"Topic_2\" \\\n        WHEN Topic_3 >= Topic_1 AND Topic_3 >= Topic_2 AND Topic_3 >= Topic_4 THEN \"Topic_3\" \\\n        WHEN Topic_4 >= Topic_1 AND Topic_4 >= Topic_2 AND Topic_4 >= Topic_3 THEN \"Topic_4\" \\\n        ELSE NULL \\\n    END AS ISSUE \\\n    FROM LDARESULTSFROMVISITS \\\n    ORDER BY NAME')\n\nloinc_lda = spark.sql('SELECT LOINC_NUM, RELATEDNAMES2, COMPONENT, \\\n    CASE \\\n        WHEN Topic_1 >= Topic_2 AND Topic_1 >= Topic_3 AND Topic_1 >= Topic_4 THEN \"Topic_1\" \\\n        WHEN Topic_2 >= Topic_1 AND Topic_2 >= Topic_3 AND Topic_2 >= Topic_4 THEN \"Topic_2\" \\\n        WHEN Topic_3 >= Topic_1 AND Topic_3 >= Topic_2 AND Topic_3 >= Topic_4 THEN \"Topic_3\" \\\n        WHEN Topic_4 >= Topic_1 AND Topic_4 >= Topic_2 AND Topic_4 >= Topic_3 THEN \"Topic_4\" \\\n        ELSE NULL \\\n    END AS ISSUE \\\n    FROM LDARESULTS')\n\nvisits_lda.createOrReplaceTempView(\"VISITS\")\nloinc_lda.createOrReplaceTempView(\"LOINC\")\n\nrecom_loinc = spark.sql(\"SELECT A.ID, A.NAME, A.DATE, A.REASON_FOR_VISIT, COLLECT_LIST(B.LOINC_NUM) AS POSSIBLE_CODES, COLLECT_LIST(B.COMPONENT) AS RELATED_TERMS FROM VISITS A JOIN LOINC B ON A.ISSUE == B.ISSUE GROUP BY ID, NAME, DATE, REASON_FOR_VISIT\")\n# recom_loinc.groupBy(\"name\", \"id\", \"date\", \"reason_for_visit\").agg(collect_list(\"LOINC_NUM\").alias(\"loinc recommendations\"))\n\ndisplay(recom_loinc)\n"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":51}],"metadata":{"name":"Applied AI on HL7 EMR Data for Categorization, Recommendation & Text Search","notebookId":903655},"nbformat":4,"nbformat_minor":0}
